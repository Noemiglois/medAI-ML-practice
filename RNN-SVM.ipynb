{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPÍTULO 5: Redes Neuronales Artificiales y Máquinas de Vectores Soporte  \n",
    "\n",
    "\n",
    "*Noemi González Lois*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MÓDULOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import sklearn\n",
    "\n",
    "# Ignorar advertencias de futuras versiones\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARGAMOS Y VISUALIZAMOS LOS DATOS DE LA BASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el **subconjunto de train y test** de la tarea 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos=['BI-RADS', 'Age', 'Shape', 'Margin', 'Density', 'Severity']\n",
    "datos_train=pd.read_csv('SubconjuntoTraining.csv', delimiter=',')\n",
    "datos_test=pd.read_csv('SubconjuntoTest.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos como target de nuestro **clasificador** la variable 'Severity' y como datos el resto de las variables.\n",
    "\n",
    "Creamos y normalizamos todos los subconjuntos de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm=MinMaxScaler()\n",
    "\n",
    "# VALIDATION\n",
    "target=datos_train.iloc[:,5:6]\n",
    "_, datos_val, _, _ = train_test_split(datos_train, target, test_size=0.2)\n",
    "\n",
    "datos_val=pd.DataFrame(norm.fit_transform(datos_val), columns=atributos)\n",
    "X_val=datos_val.drop(['Severity'],axis='columns')\n",
    "y_val=np.ravel(datos_val.iloc[:,5:6])\n",
    "\n",
    "# TRAIN\n",
    "datos_train=pd.DataFrame(norm.fit_transform(datos_train),columns=atributos)\n",
    "X_train=datos_train.drop(['Severity'],axis='columns')\n",
    "y_train=np.ravel(datos_train.iloc[:,5:6])\n",
    "\n",
    "# TEST\n",
    "datos_test=pd.DataFrame(norm.fit_transform(datos_test),columns=atributos)\n",
    "X_test=datos_test.drop(['Severity'],axis='columns')\n",
    "y_test=np.ravel(datos_test.iloc[:,5:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERCEPTRÓN MULTICAPA (MLP)\n",
    "\n",
    "Diseño del MLP utilizando las características del **espacio de observaciones original**.\n",
    "\n",
    "Los valores numéricos que representan el número de neuronas consideradas en la capa oculta son los siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1=5\n",
    "n_hidden_2=50\n",
    "n_hidden_3=500\n",
    "n_hidden_4=5000\n",
    "\n",
    "n_hidden=[n_hidden_1, n_hidden_2, n_hidden_3, n_hidden_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4;32mMLP clasiffier with 5 neurons in the hidden layer\u001b[0m\n",
      "Iteration 1, loss = 0.72450327\n",
      "Validation score: 0.538961\n",
      "Iteration 2, loss = 0.67377648\n",
      "Validation score: 0.785714\n",
      "Iteration 3, loss = 0.56923558\n",
      "Validation score: 0.785714\n",
      "Iteration 4, loss = 0.47111296\n",
      "Validation score: 0.772727\n",
      "Iteration 5, loss = 0.46256825\n",
      "Validation score: 0.785714\n",
      "Iteration 6, loss = 0.43869293\n",
      "Validation score: 0.805195\n",
      "Iteration 7, loss = 0.44299545\n",
      "Validation score: 0.805195\n",
      "Iteration 8, loss = 0.42428826\n",
      "Validation score: 0.805195\n",
      "Iteration 9, loss = 0.41782547\n",
      "Validation score: 0.831169\n",
      "Iteration 10, loss = 0.41703601\n",
      "Validation score: 0.811688\n",
      "Iteration 11, loss = 0.40756814\n",
      "Validation score: 0.785714\n",
      "Iteration 12, loss = 0.40920176\n",
      "Validation score: 0.831169\n",
      "Iteration 13, loss = 0.40827556\n",
      "Validation score: 0.831169\n",
      "Iteration 14, loss = 0.40473647\n",
      "Validation score: 0.811688\n",
      "Iteration 15, loss = 0.41252349\n",
      "Validation score: 0.837662\n",
      "Validation score did not improve more than tol=0.010000 for 5 consecutive epochs. Stopping.\n",
      "\u001b[4;32mMLP clasiffier with 50 neurons in the hidden layer\u001b[0m\n",
      "Iteration 1, loss = 0.67398692\n",
      "Validation score: 0.759740\n",
      "Iteration 2, loss = 0.50224887\n",
      "Validation score: 0.766234\n",
      "Iteration 3, loss = 0.46760769\n",
      "Validation score: 0.772727\n",
      "Iteration 4, loss = 0.44723271\n",
      "Validation score: 0.805195\n",
      "Iteration 5, loss = 0.42984944\n",
      "Validation score: 0.818182\n",
      "Iteration 6, loss = 0.42178207\n",
      "Validation score: 0.824675\n",
      "Iteration 7, loss = 0.41810925\n",
      "Validation score: 0.818182\n",
      "Iteration 8, loss = 0.41056530\n",
      "Validation score: 0.818182\n",
      "Iteration 9, loss = 0.40095994\n",
      "Validation score: 0.850649\n",
      "Iteration 10, loss = 0.40451102\n",
      "Validation score: 0.811688\n",
      "Iteration 11, loss = 0.40380022\n",
      "Validation score: 0.792208\n",
      "Iteration 12, loss = 0.40716082\n",
      "Validation score: 0.831169\n",
      "Iteration 13, loss = 0.39786071\n",
      "Validation score: 0.850649\n",
      "Iteration 14, loss = 0.39809893\n",
      "Validation score: 0.792208\n",
      "Iteration 15, loss = 0.41134806\n",
      "Validation score: 0.824675\n",
      "Validation score did not improve more than tol=0.010000 for 5 consecutive epochs. Stopping.\n",
      "\u001b[4;32mMLP clasiffier with 500 neurons in the hidden layer\u001b[0m\n",
      "Iteration 1, loss = 0.62462659\n",
      "Validation score: 0.772727\n",
      "Iteration 2, loss = 0.50924486\n",
      "Validation score: 0.779221\n",
      "Iteration 3, loss = 0.47828323\n",
      "Validation score: 0.811688\n",
      "Iteration 4, loss = 0.45704204\n",
      "Validation score: 0.805195\n",
      "Iteration 5, loss = 0.44180647\n",
      "Validation score: 0.798701\n",
      "Iteration 6, loss = 0.42620963\n",
      "Validation score: 0.837662\n",
      "Iteration 7, loss = 0.42120738\n",
      "Validation score: 0.818182\n",
      "Iteration 8, loss = 0.41709214\n",
      "Validation score: 0.824675\n",
      "Iteration 9, loss = 0.41885699\n",
      "Validation score: 0.818182\n",
      "Iteration 10, loss = 0.41174009\n",
      "Validation score: 0.798701\n",
      "Iteration 11, loss = 0.41404180\n",
      "Validation score: 0.863636\n",
      "Iteration 12, loss = 0.40933265\n",
      "Validation score: 0.818182\n",
      "Iteration 13, loss = 0.41318992\n",
      "Validation score: 0.805195\n",
      "Iteration 14, loss = 0.41062045\n",
      "Validation score: 0.844156\n",
      "Iteration 15, loss = 0.40652391\n",
      "Validation score: 0.811688\n",
      "Iteration 16, loss = 0.40954347\n",
      "Validation score: 0.844156\n",
      "Iteration 17, loss = 0.42644856\n",
      "Validation score: 0.805195\n",
      "Validation score did not improve more than tol=0.010000 for 5 consecutive epochs. Stopping.\n",
      "\u001b[4;32mMLP clasiffier with 5000 neurons in the hidden layer\u001b[0m\n",
      "Iteration 1, loss = 0.61753859\n",
      "Validation score: 0.759740\n",
      "Iteration 2, loss = 0.48958731\n",
      "Validation score: 0.759740\n",
      "Iteration 3, loss = 0.45174717\n",
      "Validation score: 0.792208\n",
      "Iteration 4, loss = 0.43480824\n",
      "Validation score: 0.798701\n",
      "Iteration 5, loss = 0.41959460\n",
      "Validation score: 0.798701\n",
      "Iteration 6, loss = 0.41429351\n",
      "Validation score: 0.792208\n",
      "Iteration 7, loss = 0.40573584\n",
      "Validation score: 0.792208\n",
      "Iteration 8, loss = 0.40256043\n",
      "Validation score: 0.772727\n",
      "Iteration 9, loss = 0.40979611\n",
      "Validation score: 0.792208\n",
      "Validation score did not improve more than tol=0.010000 for 5 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# Tabla para recoger las prestaciones en validacion\n",
    "columns = ['Loss', 'Score']\n",
    "comp_val = pd.DataFrame(columns=columns)\n",
    "\n",
    "for i in n_hidden:\n",
    "    \n",
    "    print(f'\\033[4;32mMLP clasiffier with {i} neurons in the hidden layer\\033[0m')\n",
    "\n",
    "    model = MLPClassifier(\n",
    "        activation='relu', \n",
    "        batch_size=50, \n",
    "        hidden_layer_sizes=(i,), \n",
    "        learning_rate='constant',\n",
    "        learning_rate_init=0.1, \n",
    "        max_iter=30, \n",
    "        shuffle=True, \n",
    "        solver='sgd', \n",
    "        tol=0.01, \n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.2, \n",
    "        verbose=True, \n",
    "        n_iter_no_change=5\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    loss = model.loss_  \n",
    "    comp_val.loc[i, ('Loss', 'Score')] = (loss, 'Verbose')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La pérdida ha sido añadida a la siguiente tabla pero la accuracy de cada epoch para el conjunto de validación hay que verla en la celda anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.412523</td>\n",
       "      <td>Verbose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.411348</td>\n",
       "      <td>Verbose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.426449</td>\n",
       "      <td>Verbose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.409796</td>\n",
       "      <td>Verbose</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Loss    Score\n",
       "5     0.412523  Verbose\n",
       "50    0.411348  Verbose\n",
       "500   0.426449  Verbose\n",
       "5000  0.409796  Verbose"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4;32mMLP classifier with 50 neurons in the hidden layer\u001b[0m\n",
      "Iteration 1, loss = 0.68532790\n",
      "Validation score: 0.811688\n",
      "Iteration 2, loss = 0.51653609\n",
      "Validation score: 0.837662\n",
      "Iteration 3, loss = 0.48984952\n",
      "Validation score: 0.850649\n",
      "Iteration 4, loss = 0.46840009\n",
      "Validation score: 0.857143\n",
      "Iteration 5, loss = 0.46185325\n",
      "Validation score: 0.863636\n",
      "Iteration 6, loss = 0.44184976\n",
      "Validation score: 0.870130\n",
      "Iteration 7, loss = 0.44305721\n",
      "Validation score: 0.889610\n",
      "Iteration 8, loss = 0.43689345\n",
      "Validation score: 0.857143\n",
      "Iteration 9, loss = 0.43652965\n",
      "Validation score: 0.883117\n",
      "Iteration 10, loss = 0.43107620\n",
      "Validation score: 0.863636\n",
      "Iteration 11, loss = 0.42829368\n",
      "Validation score: 0.870130\n",
      "Iteration 12, loss = 0.42901004\n",
      "Validation score: 0.850649\n",
      "Iteration 13, loss = 0.42765128\n",
      "Validation score: 0.870130\n",
      "Validation score did not improve more than tol=0.010000 for 5 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "columns = ['Accuracy', 'Recall', 'F1-score']\n",
    "comp_test = pd.DataFrame(columns=columns)\n",
    "\n",
    "print('\\033[4;32mMLP classifier with', n_hidden_2, 'neurons in the hidden layer\\033[0m')\n",
    "\n",
    "model = MLPClassifier(\n",
    "    activation='relu', \n",
    "    batch_size=50, \n",
    "    hidden_layer_sizes=(n_hidden_2,), \n",
    "    learning_rate='constant',\n",
    "    learning_rate_init=0.1, \n",
    "    max_iter=30, \n",
    "    shuffle=True, \n",
    "    solver='sgd', \n",
    "    tol=0.01, \n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.2, \n",
    "    verbose=True, \n",
    "    n_iter_no_change=5\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "cr = classification_report(y_test, predictions, output_dict=True)\n",
    "acc, rec, f1 = cr['weighted avg']['precision'], cr['weighted avg']['recall'], cr['weighted avg']['f1-score']\n",
    "\n",
    "comp_test.loc[n_hidden_2, ('Accuracy', 'Recall', 'F1-score')] = (acc, rec, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo generaliza bastate bien ya que tiene unas prestaciones bastante buenas, siendo las siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.838601</td>\n",
       "      <td>0.834197</td>\n",
       "      <td>0.834152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy    Recall  F1-score\n",
       "50  0.838601  0.834197  0.834152"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MÁQUINAS DE VECTORES SOPORTE\n",
    "\n",
    "\n",
    "Vamos a utilizar **SVC** (Support Vector Classification). La implementación de este clasificador se basa en libsvm (librería de vectores soporte para aprendizaje automático). Si quisierámos hacer casificación con bases de datos muy grandes y con kernel lineal, podríamos utilizar LinearSVC.\n",
    "\n",
    "Como vamos a utilizar un **kernel gaussiano**, utilizaremos  un kernel de radial basis function (rbf)\n",
    "\n",
    "Para la realización de este apartado vamos a utilizar la partición de **validación** para realizar validación cruzada. De esta forma, podremos ver en este subconjunto qué parámetros nos dan las mejores prestaciones para utilizarlos sobre el conjunto de test y evitar que el modelo sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns  = ['Precision','Recall','F1-score']\n",
    "comp_val = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_kernel_1='auto'\n",
    "sigma_kernel_2='scale'\n",
    "sigma_kernel_3=10\n",
    "sigma_kernel=[sigma_kernel_1,sigma_kernel_2,sigma_kernel_3]\n",
    "C1=10**(-6)\n",
    "C2=1.0\n",
    "C3=10**6\n",
    "C=[C1,C2,C3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4;35mSVM classifier with regularization parameter C=1e-06 and sigma=auto\u001b[0m\n",
      "Matriz de confusión validación:\n",
      " [[65 17]\n",
      " [12 60]]\n",
      "\n",
      "Classification report validación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.79      0.82        82\n",
      "         1.0       0.78      0.83      0.81        72\n",
      "\n",
      "    accuracy                           0.81       154\n",
      "   macro avg       0.81      0.81      0.81       154\n",
      "weighted avg       0.81      0.81      0.81       154\n",
      "\n",
      "\u001b[4;35mSVM classifier with regularization parameter C=1.0 and sigma=auto\u001b[0m\n",
      "Matriz de confusión validación:\n",
      " [[65 17]\n",
      " [12 60]]\n",
      "\n",
      "Classification report validación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.79      0.82        82\n",
      "         1.0       0.78      0.83      0.81        72\n",
      "\n",
      "    accuracy                           0.81       154\n",
      "   macro avg       0.81      0.81      0.81       154\n",
      "weighted avg       0.81      0.81      0.81       154\n",
      "\n",
      "\u001b[4;35mSVM classifier with regularization parameter C=1000000 and sigma=auto\u001b[0m\n",
      "Matriz de confusión validación:\n",
      " [[54 28]\n",
      " [ 7 65]]\n",
      "\n",
      "Classification report validación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.66      0.76        82\n",
      "         1.0       0.70      0.90      0.79        72\n",
      "\n",
      "    accuracy                           0.77       154\n",
      "   macro avg       0.79      0.78      0.77       154\n",
      "weighted avg       0.80      0.77      0.77       154\n",
      "\n",
      "\u001b[4;35mSVM classifier with regularization parameter C=1e-06 and sigma=scale\u001b[0m\n",
      "Matriz de confusión validación:\n",
      " [[61 21]\n",
      " [12 60]]\n",
      "\n",
      "Classification report validación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.74      0.79        82\n",
      "         1.0       0.74      0.83      0.78        72\n",
      "\n",
      "    accuracy                           0.79       154\n",
      "   macro avg       0.79      0.79      0.79       154\n",
      "weighted avg       0.79      0.79      0.79       154\n",
      "\n",
      "\u001b[4;35mSVM classifier with regularization parameter C=1.0 and sigma=scale\u001b[0m\n",
      "Matriz de confusión validación:\n",
      " [[65 17]\n",
      " [11 61]]\n",
      "\n",
      "Classification report validación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.79      0.82        82\n",
      "         1.0       0.78      0.85      0.81        72\n",
      "\n",
      "    accuracy                           0.82       154\n",
      "   macro avg       0.82      0.82      0.82       154\n",
      "weighted avg       0.82      0.82      0.82       154\n",
      "\n",
      "\u001b[4;35mSVM classifier with regularization parameter C=1000000 and sigma=scale\u001b[0m\n",
      "Matriz de confusión validación:\n",
      " [[72 10]\n",
      " [35 37]]\n",
      "\n",
      "Classification report validación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.88      0.76        82\n",
      "         1.0       0.79      0.51      0.62        72\n",
      "\n",
      "    accuracy                           0.71       154\n",
      "   macro avg       0.73      0.70      0.69       154\n",
      "weighted avg       0.73      0.71      0.70       154\n",
      "\n",
      "\u001b[4;35mSVM classifier with regularization parameter C=1e-06 and sigma=10\u001b[0m\n",
      "Matriz de confusión validación:\n",
      " [[15 67]\n",
      " [ 5 67]]\n",
      "\n",
      "Classification report validación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.18      0.29        82\n",
      "         1.0       0.50      0.93      0.65        72\n",
      "\n",
      "    accuracy                           0.53       154\n",
      "   macro avg       0.62      0.56      0.47       154\n",
      "weighted avg       0.63      0.53      0.46       154\n",
      "\n",
      "\u001b[4;35mSVM classifier with regularization parameter C=1.0 and sigma=10\u001b[0m\n",
      "Matriz de confusión validación:\n",
      " [[71 11]\n",
      " [25 47]]\n",
      "\n",
      "Classification report validación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.87      0.80        82\n",
      "         1.0       0.81      0.65      0.72        72\n",
      "\n",
      "    accuracy                           0.77       154\n",
      "   macro avg       0.77      0.76      0.76       154\n",
      "weighted avg       0.77      0.77      0.76       154\n",
      "\n",
      "\u001b[4;35mSVM classifier with regularization parameter C=1000000 and sigma=10\u001b[0m\n",
      "Matriz de confusión validación:\n",
      " [[73  9]\n",
      " [32 40]]\n",
      "\n",
      "Classification report validación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.89      0.78        82\n",
      "         1.0       0.82      0.56      0.66        72\n",
      "\n",
      "    accuracy                           0.73       154\n",
      "   macro avg       0.76      0.72      0.72       154\n",
      "weighted avg       0.75      0.73      0.72       154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Noemi\\anaconda3\\envs\\uni\\Lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=30).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Noemi\\anaconda3\\envs\\uni\\Lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=30).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Noemi\\anaconda3\\envs\\uni\\Lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=30).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Noemi\\anaconda3\\envs\\uni\\Lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=30).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Noemi\\anaconda3\\envs\\uni\\Lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=30).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Noemi\\anaconda3\\envs\\uni\\Lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=30).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Noemi\\anaconda3\\envs\\uni\\Lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=30).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Noemi\\anaconda3\\envs\\uni\\Lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=30).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Noemi\\anaconda3\\envs\\uni\\Lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=30).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for i in sigma_kernel:  \n",
    "    for j in C:\n",
    "        print(f'\\033[4;35mSVM classifier with regularization parameter C={j} and sigma={i}\\033[0m')\n",
    "\n",
    "        model = svm.SVC(C=j, kernel='rbf', gamma=i, max_iter=30)\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions_val = model.predict(X_val)\n",
    "\n",
    "        print('Matriz de confusión validación:\\n', confusion_matrix(y_val, predictions_val))\n",
    "        print('\\nClassification report validación:\\n', classification_report(y_val, predictions_val))\n",
    "        \n",
    "        cr = classification_report(y_val, predictions_val, output_dict=True)\n",
    "        prec, rec, f1 = cr['weighted avg']['precision'], cr['weighted avg']['recall'], cr['weighted avg']['f1-score']\n",
    "        comp_val.loc['C='+str(j)+' '+'Sigma='+str(i), ('Precision', 'Recall', 'F1-score')] = (prec, rec, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C=1e-06 Sigma=auto</th>\n",
       "      <td>0.813797</td>\n",
       "      <td>0.811688</td>\n",
       "      <td>0.811887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=1.0 Sigma=auto</th>\n",
       "      <td>0.813797</td>\n",
       "      <td>0.811688</td>\n",
       "      <td>0.811887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=1000000 Sigma=auto</th>\n",
       "      <td>0.798135</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.770502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=1e-06 Sigma=scale</th>\n",
       "      <td>0.791259</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.785796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=1.0 Sigma=scale</th>\n",
       "      <td>0.821034</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.818366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=1000000 Sigma=scale</th>\n",
       "      <td>0.726353</td>\n",
       "      <td>0.707792</td>\n",
       "      <td>0.696424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=1e-06 Sigma=10</th>\n",
       "      <td>0.633117</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.460731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=1.0 Sigma=10</th>\n",
       "      <td>0.772667</td>\n",
       "      <td>0.766234</td>\n",
       "      <td>0.762839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=1000000 Sigma=10</th>\n",
       "      <td>0.751851</td>\n",
       "      <td>0.733766</td>\n",
       "      <td>0.724836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Precision    Recall  F1-score\n",
       "C=1e-06 Sigma=auto     0.813797  0.811688  0.811887\n",
       "C=1.0 Sigma=auto       0.813797  0.811688  0.811887\n",
       "C=1000000 Sigma=auto   0.798135  0.772727  0.770502\n",
       "C=1e-06 Sigma=scale    0.791259  0.785714  0.785796\n",
       "C=1.0 Sigma=scale      0.821034  0.818182  0.818366\n",
       "C=1000000 Sigma=scale  0.726353  0.707792  0.696424\n",
       "C=1e-06 Sigma=10       0.633117  0.532468  0.460731\n",
       "C=1.0 Sigma=10         0.772667  0.766234  0.762839\n",
       "C=1000000 Sigma=10     0.751851  0.733766  0.724836"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que con valores más altos de C, las prestaciones son más bajas. Esto nos indica que nuestros datos no son linealmente separables.\n",
    "\n",
    "El valor ‘scale’ para el parámetro sigma, junto con un C muy pequeño es lo que nos da mejores prestaciones para nuestro conjunto de datos. \n",
    "\n",
    "Una vez elegidos los **mejores parámetros** de C y de sigma, los utilizamos para el **subconjunto test**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]Matriz de confusión test:\n",
      " [[79 22]\n",
      " [ 9 83]]\n",
      "\n",
      "Clasification report test:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.78      0.84       101\n",
      "         1.0       0.79      0.90      0.84        92\n",
      "\n",
      "    accuracy                           0.84       193\n",
      "   macro avg       0.84      0.84      0.84       193\n",
      "weighted avg       0.85      0.84      0.84       193\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Noemi\\anaconda3\\envs\\uni\\Lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=30).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = svm.SVC(C=10**(-6), kernel='rbf', gamma='scale', max_iter=30, verbose=True)\n",
    "model.fit(X_train, y_train)\n",
    "predictions_test = model.predict(X_test)\n",
    "\n",
    "print('Matriz de confusión test:\\n', confusion_matrix(y_test,predictions_test))\n",
    "print('\\nClasification report test:\\n', classification_report(y_test,predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPARACIÓN DE RESULTADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considero más adecuado el modelo SVM ya que sirve para separar de manera linela datos no linealmete separables inicialmente y porque además generaliza mejor.\n",
    "\n",
    "Para nuestra base de datos resulta más adecuado trabajar con los datos originales en lugar de los transformados al espacio de PCA. Esto se debe a que tenemos pocos atributos y son todos relevantes y no redundantes.\n",
    "\n",
    "\n",
    "MEJORAS\n",
    "\n",
    "Para mejorar la arquitectura MLP podríamos probar a implementar dropout para inactivar algunas neuronas y reducir así el sobreajuste, utilizar como función de coste la entropía cruzada o utilizar como función de activación la leaky ReLU.\n",
    "\n",
    "Para mejorar la arquitectura SVM podríamos probar con otras funciones kernel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
